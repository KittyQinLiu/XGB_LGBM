# XGB_LGBM

On six datasets of different scales of size, implemented three models: gradient boosted decision tree using XGboost, another gradient boosted decision tree using LightGBM, and simple neural networks. 
The goals were: 
  1) LightGBM was a fairly new package for implementing decision tree, we want to see how the accuracy and trainning time cost of using LightGBM were comparing to XGBoost. 
  2) for simple neural networks, the performance vs the troubles of tuning hyperparameters  on different scales of size of datasets. does it overfit a lot on small datasets and how to deal with it? to reach the same level of accuracy as using decision tree models, how hard do we need to adjust hyperparams and how long does the training take? See the results in the python notebook file.

code demo.ipynb demos code and results.

